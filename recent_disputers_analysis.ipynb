{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual users analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "np.random.seed(0)\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from plotnine import ggplot, aes, geom_bar, theme_minimal, labs\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import graphviz\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin'  # Adjust the path as per your Graphviz installation\n",
    "\n",
    "from sklearn.tree import _tree\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_token = 'lWYPGrEyZ0xqY7CJGTMx3DP60VBxZ21v7yrHjKguyGtQY2C5z16og6N8zu0R4Mbw'\n",
    "host = 'td.winnerstudio.vip'\n",
    "\n",
    "def pull_data(sql_script, bs_token, host):\n",
    "    data = {\n",
    "        'token':          bs_token,\n",
    "        'format':         'json_object',\n",
    "        'timeoutSeconds': 2000,\n",
    "        'sql':            sql_script\n",
    "    }\n",
    "    data = str(urlencode(data))\n",
    "    response = requests.post(f'http://{host}:8992/querySql?{data}', timeout = 1000000)\n",
    "    # Sample list of JSON strings\n",
    "    json_list = response.text.split('\\n')[1:]\n",
    "    # Convert JSON strings to dictionaries\n",
    "    dict_list = []\n",
    "    for json_str in json_list:\n",
    "        try:\n",
    "            dict_list.append(json.loads(json_str))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "    # Create a pandas DataFrame\n",
    "    data = pd.DataFrame(dict_list).sort_index(axis = 1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_within_hours(withdraw_times, first_pay_time, hours):\n",
    "    # Convert withdraw_times to datetime\n",
    "    withdraw_times_dt = pd.to_datetime(withdraw_times)\n",
    "    \n",
    "    # Convert first_pay_time to datetime (if it's not already)\n",
    "    first_pay_time_dt = pd.to_datetime(first_pay_time)\n",
    "    \n",
    "    # Calculate the end time for comparison\n",
    "    end_time = first_pay_time_dt + pd.Timedelta(hours=hours)\n",
    "    \n",
    "    # Return indexes where withdraw_times_dt is within the specified hours\n",
    "    return [i for i, time in enumerate(withdraw_times_dt) if time <= end_time]\n",
    "\n",
    "def extract_within_hours(withdraw_times, first_pay_time, hours):\n",
    "    # Convert withdraw_times to datetime\n",
    "    withdraw_times_dt = pd.to_datetime(withdraw_times)\n",
    "    \n",
    "    # Convert first_pay_time to datetime (if it's not already)\n",
    "    first_pay_time_dt = pd.to_datetime(first_pay_time)\n",
    "    \n",
    "    # Calculate the end time for comparison\n",
    "    end_time = first_pay_time_dt + pd.Timedelta(hours=hours)\n",
    "    \n",
    "    # Return indexes where withdraw_times_dt is within the specified hours\n",
    "    return [i for i, time in enumerate(withdraw_times_dt) if time <= end_time]\n",
    "\n",
    "def sum_withdraws_within_hours(withdraw_history, indexes):\n",
    "    withdraw_history = str(withdraw_history).strip('[]')\n",
    "    withdraw_values_list = [str(item.strip()) for item in withdraw_history.split(',')]\n",
    "    \n",
    "    # Ensure all values are numeric and calculate sum for selected indexes\n",
    "    selected_withdraws = [float(withdraw_values_list[i].replace('[', '').replace(']', '')) for i in indexes \n",
    "                            if i < len(withdraw_values_list) \n",
    "                            and withdraw_values_list[i] is not None \n",
    "                            and withdraw_values_list[i].replace('[', '').replace(']', '') != ''\n",
    "                            and withdraw_values_list[i].replace('[', '').replace(']', '') != '.']\n",
    "    \n",
    "    return sum(selected_withdraws)\n",
    "\n",
    "def max_payments_in_window(payment_times, window_minutes):\n",
    "    max_count = 0\n",
    "    payment_times = pd.to_datetime(payment_times)\n",
    "    for i, start_time in enumerate(payment_times):\n",
    "        end_time = start_time + pd.Timedelta(minutes=window_minutes)\n",
    "        count = np.sum((payment_times >= start_time) & (payment_times <= end_time))\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "    return max_count\n",
    "\n",
    "def extract_second_payment(x):\n",
    "    # Remove brackets and replace 'null' with 'NaN'\n",
    "    cleaned = x.replace('[', '').replace(']', '').replace('null', 'NaN').split(',')\n",
    "    # Check if there is a second element\n",
    "    if len(cleaned) > 1:\n",
    "        return float(cleaned[1])\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract_third_payment(x):\n",
    "    # Remove brackets and replace 'null' with 'NaN'\n",
    "    cleaned = x.replace('[', '').replace(']', '').replace('null', 'NaN').split(',')\n",
    "    # Check if there is a second element\n",
    "    if len(cleaned) > 1:\n",
    "        return float(cleaned[2])\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def calculate_minutes_diff(start, end):\n",
    "    if pd.notna(start) and pd.notna(end):\n",
    "        return (end - start).total_seconds() / 60.0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_unique_values(lst):\n",
    "    return list(set(lst))\n",
    "\n",
    "def extract_datetime(x, index):\n",
    "    dates = x  # Convert JSON string to list\n",
    "    if len(dates) > index:\n",
    "        return pd.to_datetime(str(dates[index])[1:-1])\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# General function to sum payments within specified hours using the extracted indexes\n",
    "def sum_payments_within_hours(payment_history, indexes):\n",
    "    payment_history = str(payment_history).strip('[]')\n",
    "    payment_values_list = [str(item.strip()) for item in payment_history.split(',')]\n",
    "    selected_payments = [float(payment_values_list[i].replace('[', '').replace(']', '')) for i in indexes \n",
    "                            if i < len(payment_values_list) \n",
    "                            and payment_values_list[i] is not None \n",
    "                            and payment_values_list[i].replace('[', '').replace(']', '') != ''\n",
    "                            and payment_values_list[i].replace('[', '').replace(']', '') != '.']\n",
    "    \n",
    "    return sum(selected_payments)\n",
    "\n",
    "# Function to convert the string list to a list of datetime objects\n",
    "def convert_to_datetime_list(string):\n",
    "    string = str(string).strip('[]').replace(\"'\", '\"').replace('\"', '').replace(\" 2023\", '2023').replace(\" 2024\", '2024').replace(\"     2023\", '2023')\n",
    "\n",
    "    string = string.strip()\n",
    "    date_strs = string.split(',')\n",
    "    date_strs = [date_str.strip() for date_str in date_strs]\n",
    "    return pd.to_datetime(date_strs, format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Define the model \n",
    "def create_model(criterion, depth, leaf_size):\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('regressor', DecisionTreeClassifier(criterion = criterion, max_depth = depth, min_samples_leaf = leaf_size))\n",
    "    ])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "def traverse_tree(tree, node_id = 0, depth = 0, dataset = None, path = None):\n",
    "\n",
    "    if dataset is None:\n",
    "        dataset = pd.DataFrame(columns=[\"Event\", \"Disputer rate\"])\n",
    "    \n",
    "    if path is None:\n",
    "        path = []\n",
    "\n",
    "    if tree.children_left[node_id] != _tree.TREE_LEAF:\n",
    "    # if node_id != _tree.TREE_LEAF:\n",
    "        feature_name = X_train.columns[tree.feature[node_id]]\n",
    "        split_value = tree.threshold[node_id]\n",
    "        \n",
    "        path_left = path + [(f\"{feature_name} â‰¤ {round(split_value, 3)}\")]\n",
    "        dataset = traverse_tree(tree, tree.children_left[node_id], depth + 1, dataset, path_left)\n",
    "        \n",
    "        path_right = path + [(f\"{feature_name} > {round(split_value, 3)}\")]\n",
    "        dataset = traverse_tree(tree, tree.children_right[node_id], depth + 1, dataset, path_right) \n",
    "        # print('function here is okay')    \n",
    "    \n",
    "    else:\n",
    "        leaf_size = np.sum(tree.value[node_id])\n",
    "        # print('leaf_size', leaf_size)\n",
    "        # print('else here is okay')\n",
    "        # print(len(tree.value[node_id][0]))\n",
    "        if len(tree.value[node_id][0]) > 1: \n",
    "            true_cases = tree.value[node_id][0][1]\n",
    "        else: \n",
    "            true_cases = 0\n",
    "        # print('else here is okay oh oh')\n",
    "        # proportion_true = tree.value[node_id][0][1] / np.sum(tree.value[node_id])\n",
    "        proportion_true = true_cases / leaf_size\n",
    "        event = \" & \".join([f\"{event_name}\" if \" > \" in event_name else f\"{event_name}\" for event_name in path])\n",
    "        # print('æœ‰é—®é¢˜')\n",
    "        dataset = pd.concat([dataset, pd.DataFrame({\"Event\": [event], \"Disputer rate\": [proportion_true],\n",
    "                                                    \"Payers\": [leaf_size], \"Disputers\": [true_cases]})], ignore_index=True)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "select  \n",
      "    \"#account_id\"\n",
      "    , \"$part_event\"\n",
      "    , min(\"#event_time\") as first_dispute \n",
      "    , max(\"#event_time\") as last_dispute\n",
      "    , count(distinct \"#event_time\") as total_disputes\n",
      "from ta.v_event_59 \n",
      "    where \"$part_event\" in ('pay_dispute', 'fraud') \n",
      "group by 1,2\n",
      "order by 1,2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So, you need to investigate how to increase the number of users across your platform. You need to understand the relevant parameters that will increase the performance of your game \n",
    "day = 7\n",
    "version = 1\n",
    "\n",
    "final_query = f\"\"\"\n",
    "\n",
    "select  \n",
    "    \"#account_id\"\n",
    "    , \"$part_event\"\n",
    "    , min(\"#event_time\") as first_dispute \n",
    "    , max(\"#event_time\") as last_dispute\n",
    "    , count(distinct \"#event_time\") as total_disputes\n",
    "from ta.v_event_59 \n",
    "    where \"$part_event\" in ('pay_dispute', 'fraud') \n",
    "group by 1,2\n",
    "order by 1,2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pull_data(final_query, bs_token, host)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_user_behavior(df):\n",
    "    user_analysis = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        account_id = row['#account_id']\n",
    "        sum_bet_7 = row['sum_bet_day_7']\n",
    "        avg_bet_7 = row['avg_bet_day_7']\n",
    "        max_bet_7 = row['max_bet_day_7']\n",
    "        win_bet_7 = row['win_bet_day_7']\n",
    "        \n",
    "        behavior_notes = []\n",
    "\n",
    "        # Check for high betting behavior\n",
    "        if max_bet_7 > (avg_bet_7 * 10):\n",
    "            behavior_notes.append(f\"High single bet compared to average betting behavior (Max bet: {max_bet_7}, Avg bet: {avg_bet_7}).\")\n",
    "\n",
    "        # Check for consistent losses\n",
    "        if win_bet_7 < 0:\n",
    "            behavior_notes.append(f\"User consistently losing in the last 7 days (Total loss: {win_bet_7}).\")\n",
    "\n",
    "        # Check for sudden changes in betting patterns\n",
    "        if row['sum_bet_day_1'] != row['sum_bet_day_7']:\n",
    "            behavior_notes.append(f\"Betting pattern changed from Day 1 to Day 7 (Day 1 total bet: {row['sum_bet_day_1']}, Day 7 total bet: {sum_bet_7}).\")\n",
    "\n",
    "        # Check for high total bet amount that could lead to disputes\n",
    "        if sum_bet_7 > 10000:  # arbitrary threshold for potential dispute risk\n",
    "            behavior_notes.append(f\"High total bet in the last 7 days: {sum_bet_7}. This might be a reason for a dispute.\")\n",
    "        \n",
    "        if behavior_notes:\n",
    "            user_analysis.append((account_id, behavior_notes))\n",
    "    \n",
    "    return user_analysis\n",
    "\n",
    "# Perform the analysis on the dataset\n",
    "user_behaviors = analyze_user_behavior(data)\n",
    "\n",
    "# Display the first few user behavior analyses\n",
    "user_behaviors[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
