{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual users analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete Analysis of 20 or More Risk Cases: This task involves completing an in-depth analysis of at least 20 cases involving various types of risks. The goal is probably to identify trends, understand common risk factors, and provide insights that could help prevent or better manage these risks in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "DisPuting information\n",
    "- Fraud, disPute or both? \n",
    "- Time\n",
    "- time since first Payment and Last Payment\n",
    "- What did he do on his last day before the first DisP\n",
    "- Last money game and time to dis\n",
    "- Payment methods used and ayment methods disPuted \n",
    "\n",
    "User \n",
    "- Does he bet a lot \n",
    "- Are there any strong balance droPs? How does he react?\n",
    "- Does he Play during abnormal times?\n",
    "- How many withdrawal failures has he had? \n",
    "- Does he Play at times that are abnormal? comPared with other users \n",
    "- Has he interacted with customer service? \n",
    "- How long did it take him to make the first Payment? (Is it very fast, is it very soon?)\n",
    "- Has there been any issue withdrawing, or making a Payment? \n",
    "- Has he aPPlied many times for withdraw? has he succeded at all? \n",
    "\n",
    "Action\n",
    "- Has he made any abnormal bet? (In terms of himself, or in terms of other users)\n",
    "- Does he Play at times that are abnormal to himself?\n",
    "- Has he made an abnormal Payment (Any ayment that is sisgnificatively large)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to train ChatGPT on having a full understanding of the database. \n",
    "\n",
    "1. Share the variable dictionary\n",
    "2. Share some cases and exPlain the bundle IDs \n",
    "3. Share some queries and tell show the outPut and what they do \n",
    "4. Ask him how he would do some analyses and get him ready fast. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also want him to do some queries and Plots some results for each selected user. Also incorPorate Colin's information. At the end of the day you can do a similar job as Colin.\n",
    "\n",
    "(The reason is because I want to do this everymonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "np.random.seed(0)\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from plotnine import ggplot, aes, geom_bar, theme_minimal, labs\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import graphviz\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin'  # Adjust the path as per your Graphviz installation\n",
    "\n",
    "from sklearn.tree import _tree\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_token = 'lWYPGrEyZ0xqY7CJGTMx3DP60VBxZ21v7yrHjKguyGtQY2C5z16og6N8zu0R4Mbw'\n",
    "host = 'td.winnerstudio.vip'\n",
    "\n",
    "def pull_data(sql_script, bs_token, host):\n",
    "    data = {\n",
    "        'token':          bs_token,\n",
    "        'format':         'json_object',\n",
    "        'timeoutSeconds': 2000,\n",
    "        'sql':            sql_script\n",
    "    }\n",
    "    data = str(urlencode(data))\n",
    "    response = requests.post(f'http://{host}:8992/querySql?{data}', timeout = 1000000)\n",
    "    # Sample list of JSON strings\n",
    "    json_list = response.text.split('\\n')[1:]\n",
    "    # Convert JSON strings to dictionaries\n",
    "    dict_list = []\n",
    "    for json_str in json_list:\n",
    "        try:\n",
    "            dict_list.append(json.loads(json_str))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "    # Create a pandas DataFrame\n",
    "    data = pd.DataFrame(dict_list).sort_index(axis = 1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_within_hours(withdraw_times, first_pay_time, hours):\n",
    "    # Convert withdraw_times to datetime\n",
    "    withdraw_times_dt = pd.to_datetime(withdraw_times)\n",
    "    \n",
    "    # Convert first_pay_time to datetime (if it's not already)\n",
    "    first_pay_time_dt = pd.to_datetime(first_pay_time)\n",
    "    \n",
    "    # Calculate the end time for comparison\n",
    "    end_time = first_pay_time_dt + pd.Timedelta(hours=hours)\n",
    "    \n",
    "    # Return indexes where withdraw_times_dt is within the specified hours\n",
    "    return [i for i, time in enumerate(withdraw_times_dt) if time <= end_time]\n",
    "\n",
    "def extract_within_hours(withdraw_times, first_pay_time, hours):\n",
    "    # Convert withdraw_times to datetime\n",
    "    withdraw_times_dt = pd.to_datetime(withdraw_times)\n",
    "    \n",
    "    # Convert first_pay_time to datetime (if it's not already)\n",
    "    first_pay_time_dt = pd.to_datetime(first_pay_time)\n",
    "    \n",
    "    # Calculate the end time for comparison\n",
    "    end_time = first_pay_time_dt + pd.Timedelta(hours=hours)\n",
    "    \n",
    "    # Return indexes where withdraw_times_dt is within the specified hours\n",
    "    return [i for i, time in enumerate(withdraw_times_dt) if time <= end_time]\n",
    "\n",
    "def sum_withdraws_within_hours(withdraw_history, indexes):\n",
    "    withdraw_history = str(withdraw_history).strip('[]')\n",
    "    withdraw_values_list = [str(item.strip()) for item in withdraw_history.split(',')]\n",
    "    \n",
    "    # Ensure all values are numeric and calculate sum for selected indexes\n",
    "    selected_withdraws = [float(withdraw_values_list[i].replace('[', '').replace(']', '')) for i in indexes \n",
    "                            if i < len(withdraw_values_list) \n",
    "                            and withdraw_values_list[i] is not None \n",
    "                            and withdraw_values_list[i].replace('[', '').replace(']', '') != ''\n",
    "                            and withdraw_values_list[i].replace('[', '').replace(']', '') != '.']\n",
    "    \n",
    "    return sum(selected_withdraws)\n",
    "\n",
    "def max_payments_in_window(payment_times, window_minutes):\n",
    "    max_count = 0\n",
    "    payment_times = pd.to_datetime(payment_times)\n",
    "    for i, start_time in enumerate(payment_times):\n",
    "        end_time = start_time + pd.Timedelta(minutes=window_minutes)\n",
    "        count = np.sum((payment_times >= start_time) & (payment_times <= end_time))\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "    return max_count\n",
    "\n",
    "def extract_second_payment(x):\n",
    "    # Remove brackets and replace 'null' with 'NaN'\n",
    "    cleaned = x.replace('[', '').replace(']', '').replace('null', 'NaN').split(',')\n",
    "    # Check if there is a second element\n",
    "    if len(cleaned) > 1:\n",
    "        return float(cleaned[1])\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract_third_payment(x):\n",
    "    # Remove brackets and replace 'null' with 'NaN'\n",
    "    cleaned = x.replace('[', '').replace(']', '').replace('null', 'NaN').split(',')\n",
    "    # Check if there is a second element\n",
    "    if len(cleaned) > 1:\n",
    "        return float(cleaned[2])\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def calculate_minutes_diff(start, end):\n",
    "    if pd.notna(start) and pd.notna(end):\n",
    "        return (end - start).total_seconds() / 60.0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_unique_values(lst):\n",
    "    return list(set(lst))\n",
    "\n",
    "def extract_datetime(x, index):\n",
    "    dates = x  # Convert JSON string to list\n",
    "    if len(dates) > index:\n",
    "        return pd.to_datetime(str(dates[index])[1:-1])\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# General function to sum payments within specified hours using the extracted indexes\n",
    "def sum_payments_within_hours(payment_history, indexes):\n",
    "    payment_history = str(payment_history).strip('[]')\n",
    "    payment_values_list = [str(item.strip()) for item in payment_history.split(',')]\n",
    "    selected_payments = [float(payment_values_list[i].replace('[', '').replace(']', '')) for i in indexes \n",
    "                            if i < len(payment_values_list) \n",
    "                            and payment_values_list[i] is not None \n",
    "                            and payment_values_list[i].replace('[', '').replace(']', '') != ''\n",
    "                            and payment_values_list[i].replace('[', '').replace(']', '') != '.']\n",
    "    \n",
    "    return sum(selected_payments)\n",
    "\n",
    "# Function to convert the string list to a list of datetime objects\n",
    "def convert_to_datetime_list(string):\n",
    "    string = str(string).strip('[]').replace(\"'\", '\"').replace('\"', '').replace(\" 2023\", '2023').replace(\" 2024\", '2024').replace(\"     2023\", '2023')\n",
    "\n",
    "    string = string.strip()\n",
    "    date_strs = string.split(',')\n",
    "    date_strs = [date_str.strip() for date_str in date_strs]\n",
    "    return pd.to_datetime(date_strs, format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Define the model \n",
    "def create_model(criterion, depth, leaf_size):\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('regressor', DecisionTreeClassifier(criterion = criterion, max_depth = depth, min_samples_leaf = leaf_size))\n",
    "    ])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "def traverse_tree(tree, node_id = 0, depth = 0, dataset = None, path = None):\n",
    "\n",
    "    if dataset is None:\n",
    "        dataset = pd.DataFrame(columns=[\"Event\", \"Disputer rate\"])\n",
    "    \n",
    "    if path is None:\n",
    "        path = []\n",
    "\n",
    "    if tree.children_left[node_id] != _tree.TREE_LEAF:\n",
    "    # if node_id != _tree.TREE_LEAF:\n",
    "        feature_name = X_train.columns[tree.feature[node_id]]\n",
    "        split_value = tree.threshold[node_id]\n",
    "        \n",
    "        path_left = path + [(f\"{feature_name} ≤ {round(split_value, 3)}\")]\n",
    "        dataset = traverse_tree(tree, tree.children_left[node_id], depth + 1, dataset, path_left)\n",
    "        \n",
    "        path_right = path + [(f\"{feature_name} > {round(split_value, 3)}\")]\n",
    "        dataset = traverse_tree(tree, tree.children_right[node_id], depth + 1, dataset, path_right) \n",
    "        # print('function here is okay')    \n",
    "    \n",
    "    else:\n",
    "        leaf_size = np.sum(tree.value[node_id])\n",
    "        # print('leaf_size', leaf_size)\n",
    "        # print('else here is okay')\n",
    "        # print(len(tree.value[node_id][0]))\n",
    "        if len(tree.value[node_id][0]) > 1: \n",
    "            true_cases = tree.value[node_id][0][1]\n",
    "        else: \n",
    "            true_cases = 0\n",
    "        # print('else here is okay oh oh')\n",
    "        # proportion_true = tree.value[node_id][0][1] / np.sum(tree.value[node_id])\n",
    "        proportion_true = true_cases / leaf_size\n",
    "        event = \" & \".join([f\"{event_name}\" if \" > \" in event_name else f\"{event_name}\" for event_name in path])\n",
    "        # print('有问题')\n",
    "        dataset = pd.concat([dataset, pd.DataFrame({\"Event\": [event], \"Disputer rate\": [proportion_true],\n",
    "                                                    \"Payers\": [leaf_size], \"Disputers\": [true_cases]})], ignore_index=True)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "select  \n",
      "    \"#account_id\"\n",
      "    , \"$part_event\"\n",
      "    , min(\"#event_time\") as first_dispute \n",
      "    , max(\"#event_time\") as last_dispute\n",
      "    , count(distinct \"#event_time\") as total_disputes\n",
      "from ta.v_event_59 \n",
      "    where \"$part_event\" in ('pay_dispute', 'fraud') \n",
      "group by 1,2\n",
      "order by 1,2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So, you need to investigate how to increase the number of users across your platform. You need to understand the relevant parameters that will increase the performance of your game \n",
    "day = 7\n",
    "version = 1\n",
    "\n",
    "final_query = f\"\"\"\n",
    "\n",
    "select  \n",
    "    \"#account_id\"\n",
    "    , \"$part_event\"\n",
    "    , min(\"#event_time\") as first_dispute \n",
    "    , max(\"#event_time\") as last_dispute\n",
    "    , count(distinct \"#event_time\") as total_disputes\n",
    "from ta.v_event_59 \n",
    "    where \"$part_event\" in ('pay_dispute', 'fraud') \n",
    "group by 1,2\n",
    "order by 1,2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pull_data(final_query, bs_token, host)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Win11/Downloads/merged_analysis_output.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the uploaded CSV file\n",
    "uploaded_file_path = \"C:/Users/Win11/Downloads/20241118_123232_04413_ptsf3.csv\"\n",
    "uploaded_data = pd.read_csv(uploaded_file_path)\n",
    "\n",
    "# Create the analysis DataFrame from the above table\n",
    "analysis_data = pd.DataFrame([\n",
    "    {\"#account_id\": 108031994, \"bundle_id\": \"com.jugglesleep.iw\", \"analysis\": \"Low activity, single dispute, no significant deposits or gameplay. Likely misunderstanding or exploratory use of the platform.\"},\n",
    "    {\"#account_id\": 138008153, \"bundle_id\": \"com.fortune1.splash\", \"analysis\": \"Significant gameplay (1562 spins) with small deposits and a single dispute. Likely caused by minor misunderstandings during gameplay or deposit issues.\"},\n",
    "    {\"#account_id\": 108031767, \"bundle_id\": \"com.jugglesleep.iw\", \"analysis\": \"Moderate gameplay and deposits with two disputes. Possible dissatisfaction with deposit outcomes or misunderstanding during transactions.\"},\n",
    "    {\"#account_id\": 138009403, \"bundle_id\": \"com.fortune1.splash\", \"analysis\": \"High gameplay and significant net losses (-1938.35). Five disputes suggest dissatisfaction with outcomes or attempts to recover losses through disputes.\"},\n",
    "    {\"#account_id\": 138010484, \"bundle_id\": \"com.fortune1.splash\", \"analysis\": \"Minimal gameplay and deposits. Single dispute likely due to a misunderstanding or isolated incident.\"},\n",
    "    {\"#account_id\": 138010808, \"bundle_id\": \"com.fortune1.splash\", \"analysis\": \"Moderate gameplay but minimal deposits. Dispute after significant inactivity could indicate confusion about earlier transactions.\"},\n",
    "    {\"#account_id\": 108030681, \"bundle_id\": \"com.jugglesleep.iw\", \"analysis\": \"Extremely low activity with only one deposit and one dispute. Likely a casual or exploratory user with minimal platform engagement.\"},\n",
    "    {\"#account_id\": 118032090, \"bundle_id\": \"com.muggle4.bigfortune\", \"analysis\": \"Significant deposits but with a low deposit success rate (23%). Dispute likely stems from frustrations with failed deposits or unclear transaction outcomes.\"},\n",
    "    {\"#account_id\": 108033111, \"bundle_id\": \"com.jugglesleep.iw\", \"analysis\": \"Minimal activity and a single dispute. Likely due to confusion or isolated dissatisfaction with the platform's transactions.\"},\n",
    "    {\"#account_id\": 108027827, \"bundle_id\": \"com.jugglesleep.iw\", \"analysis\": \"Moderate deposits but high failed deposit rate. Dispute likely caused by failed transactions or perceived errors during gameplay or deposits.\"},\n",
    "    {\"#account_id\": 118032606, \"bundle_id\": \"com.muggle4.bigfortune\", \"analysis\": \"High activity (6677 spins) but numerous disputes (12). Likely indicates dissatisfaction with outcomes, unclear transactions, or attempts to recover losses.\"},\n",
    "    {\"#account_id\": 108029248, \"bundle_id\": \"com.jugglesleep.iw\", \"analysis\": \"Significant gameplay and deposits with four disputes. Likely driven by dissatisfaction with losses or perceived unfairness in deposit outcomes.\"},\n",
    "    {\"#account_id\": 138009990, \"bundle_id\": \"com.fortune2.tide\", \"analysis\": \"Minimal activity with a single dispute. Likely due to a one-off misunderstanding or isolated dissatisfaction.\"},\n",
    "    {\"#account_id\": 138009045, \"bundle_id\": \"com.fortune1.splash\", \"analysis\": \"High deposits and gameplay but significant losses. Five disputes likely stem from dissatisfaction with financial outcomes or frustration from perceived platform issues.\"},\n",
    "    {\"#account_id\": 138010150, \"bundle_id\": \"com.fortune2.tide\", \"analysis\": \"Moderate activity with one dispute. Likely caused by minor dissatisfaction with outcomes or misunderstandings.\"},\n",
    "    {\"#account_id\": 108027003, \"bundle_id\": \"com.jugglesleep.iw\", \"analysis\": \"Moderate gameplay and deposits with three disputes. Likely driven by dissatisfaction with losses or unclear outcomes.\"},\n",
    "    {\"#account_id\": 138010391, \"bundle_id\": \"com.fortune2.tide\", \"analysis\": \"Moderate gameplay but minimal deposits. Two disputes could be related to dissatisfaction with deposit success rate or unclear outcomes.\"},\n",
    "    {\"#account_id\": 138008969, \"bundle_id\": \"com.fortune1.splash\", \"analysis\": \"Low deposit success rate with significant losses. Dispute likely stems from frustration over unclear or failed deposit transactions.\"},\n",
    "    {\"#account_id\": 138009809, \"bundle_id\": \"com.fortune2.tide\", \"analysis\": \"High gameplay but minimal deposits. Likely caused by isolated dissatisfaction or misunderstanding about deposit-related processes.\"},\n",
    "    {\"#account_id\": 118033014, \"bundle_id\": \"com.muggle4.bigfortune\", \"analysis\": \"High activity and disputes (5 fraud cases). Likely indicates dissatisfaction with financial outcomes or perceived unfairness in transactions.\"},\n",
    "    {\"#account_id\": 108030684, \"bundle_id\": \"com.jugglesleep.iw\", \"analysis\": \"Significant disputes (8 fraud cases) and moderate deposits. Likely indicates dissatisfaction or attempts to recover losses through disputes.\"},\n",
    "    {\"#account_id\": 138009805, \"bundle_id\": \"com.fortune2.tide\", \"analysis\": \"Low activity with two disputes. Likely stems from isolated dissatisfaction or confusion regarding transaction outcomes.\"}\n",
    "])\n",
    "\n",
    "# Join uploaded data with analysis data on account_id\n",
    "merged_data = uploaded_data.merge(analysis_data, on='#account_id', how='left')\n",
    "\n",
    "# Save the merged data to a CSV\n",
    "output_file_path = 'C:/Users/Win11/Downloads/merged_analysis_output.csv'\n",
    "merged_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_user_behavior(df):\n",
    "    user_analysis = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        account_id = row['#account_id']\n",
    "        sum_bet_7 = row['sum_bet_day_7']\n",
    "        avg_bet_7 = row['avg_bet_day_7']\n",
    "        max_bet_7 = row['max_bet_day_7']\n",
    "        win_bet_7 = row['win_bet_day_7']\n",
    "        \n",
    "        behavior_notes = []\n",
    "\n",
    "        # Check for high betting behavior\n",
    "        if max_bet_7 > (avg_bet_7 * 10):\n",
    "            behavior_notes.append(f\"High single bet compared to average betting behavior (Max bet: {max_bet_7}, Avg bet: {avg_bet_7}).\")\n",
    "\n",
    "        # Check for consistent losses\n",
    "        if win_bet_7 < 0:\n",
    "            behavior_notes.append(f\"User consistently losing in the last 7 days (Total loss: {win_bet_7}).\")\n",
    "\n",
    "        # Check for sudden changes in betting patterns\n",
    "        if row['sum_bet_day_1'] != row['sum_bet_day_7']:\n",
    "            behavior_notes.append(f\"Betting pattern changed from Day 1 to Day 7 (Day 1 total bet: {row['sum_bet_day_1']}, Day 7 total bet: {sum_bet_7}).\")\n",
    "\n",
    "        # Check for high total bet amount that could lead to disputes\n",
    "        if sum_bet_7 > 10000:  # arbitrary threshold for potential dispute risk\n",
    "            behavior_notes.append(f\"High total bet in the last 7 days: {sum_bet_7}. This might be a reason for a dispute.\")\n",
    "        \n",
    "        if behavior_notes:\n",
    "            user_analysis.append((account_id, behavior_notes))\n",
    "    \n",
    "    return user_analysis\n",
    "\n",
    "# Perform the analysis on the dataset\n",
    "user_behaviors = analyze_user_behavior(data)\n",
    "\n",
    "# Display the first few user behavior analyses\n",
    "user_behaviors[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
